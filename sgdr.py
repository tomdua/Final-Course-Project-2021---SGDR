# -*- coding: utf-8 -*-
"""sgdr.ipynb

Automatically generated by Colaboratory.

Original file is located at
    https://colab.research.google.com/drive/1TDbkKr6TpWjlGpC4Wcyt7UsLrk7OTEx3

# imports
"""

import tensorflow as tf
physical_devices = tf.config.list_physical_devices('GPU')
tf.config.experimental.set_memory_growth(physical_devices[0], enable=True)

!pip install pandas
!pip install numpy
!pip install sklearn
!pip install keras
!pip install keras-tuner --upgrade
!pip install bayesian-optimization

import sklearn
from sklearn import metrics
import timeit
import pandas as pd
import numpy as np
from tensorflow import keras
from keras.callbacks import LearningRateScheduler
from sklearn.preprocessing import LabelEncoder
from keras.utils import np_utils
from sklearn.metrics import mean_squared_error
from sklearn.metrics import mean_absolute_error
from sklearn.tree import DecisionTreeRegressor  
from sklearn.metrics import r2_score
from sklearn.linear_model import LinearRegression
from sklearn.preprocessing import PolynomialFeatures
from sklearn.svm import SVR
import warnings
warnings.filterwarnings('ignore')
from keras.models import Sequential,model_from_json
from keras.layers import Dense
from keras.optimizers import RMSprop
import numpy
from sklearn.model_selection import RandomizedSearchCV
from bayes_opt import BayesianOptimization
from tensorflow.keras.models import Sequential
from keras.layers import Dense
from keras.wrappers.scikit_learn import KerasClassifier
from sklearn.model_selection import KFold
from sklearn.model_selection import train_test_split
import keras_tuner as kt
from tensorflow import keras
import itertools
import random
import csv

"""# Data"""

from google.colab import drive
drive.mount('/content/drive')

csv_path="/content/drive/MyDrive/Colab Notebooks/SGDR/final_table.csv"
df=pd.read_csv(csv_path)
df.to_csv(csv_path, encoding='utf-8', index=False)

def split_data(df):
  df = df.fillna(0)
  x = df[:]
  x = x.drop(['class'],axis=1).to_numpy()
  y = df[:]['class']

  encoder = LabelEncoder()
  encoder.fit(y)
  encoded_Y = encoder.transform(y)
  # convert integers to dummy variables (i.e. one hot encoded)
  y = np_utils.to_categorical(encoded_Y)

  # print(x)
  # print(x.shape[1])
  # # print(y)
  # print(y.shape[1])

  # print(len(x))
  return x,y

def read_data(data_name):
  path="/content/drive/MyDrive/Colab Notebooks/SGDR/final_porject_data/add/"+data_name
  df=pd.read_csv(path)
  # print(df.shape)
  # print(df.head())
  x,y=split_data(df)
  return x,y

import os
mylist = os.listdir('/content/drive/MyDrive/Colab Notebooks/SGDR/final_porject_data/add/')
best_warm={}
counter = 0

for element in mylist:
  print('counter data: ',counter)
  print('dataname: ' ,element)
  x,y = read_data(element)
  best_model,best_hp=run(x,y)
  print('counter data: ',counter)
  print('dataname: ' ,element)
  print('best model: ',best_model)
  print('best hp: ',best_hp)
  counter+=1
  
  Algorithm_Name = 'SGDR'
  cross_Validation = list(range(1, 11))
  # row_list = ["Dataset Name","Algorithm Name","Cross Validation [1-10]",'Hyper-Parameters Values','Accuracy','Precision','Training Time','Inference Time']
  row_list = []
  for i in range (0,10):
    precision = dict(filter(lambda item: 'precision' in item[0], best_model[i].items()))
    precision_val = list(precision.values())[0][-1:]
    temp = [element, Algorithm_Name, cross_Validation[i],best_hp[i],best_model[i]['accuracy'][-1:], precision_val, sum(best_model[i]['train_time']), sum(best_model[0]['train_time'])]
    row_list.append(temp)                                                                                                                 
                                                                                                                        
  with open('/content/drive/MyDrive/Colab Notebooks/SGDR/final_table.csv', 'a', newline='') as file:
      writer = csv.writer(file)
      writer.writerows(row_list)

  best_warm[element]=(best_model,best_hp)

"""Split Dataset

# kfold validation

# **Stage 1**

hyperParameters:
"""

array_T0 = list(range(1, 51))
array_Tmult =[1,2]
array_lr = list(np.arange(0.00001, 0.1, 0.001))

hp_warm = list(itertools.product(array_T0, array_Tmult,array_lr))
random.shuffle(hp_warm)
hp_warm = hp_warm[:50]
# print(hp_warm)

array_epoch_update_lr = list(range(1, 51))
hp_basic = list(itertools.product(array_epoch_update_lr, array_lr))
random.shuffle(hp_basic)
hp_basic = hp_basic[:50]#epoch, lr
# hp_basic

hp={"basic":hp_basic, "warm": hp_warm}

reduce_lr_epoch = 10
reduce_factor = 10

def lr_scheduler_basic(epoch, lr):
    if epoch == reduce_lr_epoch:
      print("Reduce lr from {} to {}".format(lr, lr/reduce_factor))
      return lr/reduce_factor
    else:
      return lr

t0 = 10
ti = t0
tmult = 1
lr_warmup_current = 0.02 #max
lr_warmup_next = lr_warmup_current
tcur = 1
lr_min = 0

def lr_scheduler_warm(epoch, lr):
    
    global tcur
    tcur += 1
    global ti
    if tcur > ti:
        ti = int(tmult * ti)
        tcur = 1
        # global lr_warmup_current
        # lr_warmup_current = lr_warmup_next
    lr = float(lr_min + (lr_warmup_current - lr_min) * (1 + np.cos(tcur/ti*np.pi)) / 2.0)
    return lr

"""model sgdr

Model Stage 1  - SGDR: Stochastic Gradient Descent

random search:

Regular - without optimizer
"""

def build_model(lr,x_size,y_size):
  
  act_label=[]
  pred_label=[]
  model = Sequential()
  model.add(Dense(256, activation='relu', input_shape=(x_size,)))
  model.add(Dense(256, activation='relu'))
  model.add(Dense(256, activation='relu'))
  model.add(Dense(256, activation='relu'))
  model.add(Dense(y_size, activation='softmax'))
  optimizer = keras.optimizers.Adam(lr)
  model.compile(loss='categorical_crossentropy', optimizer=optimizer, metrics=['accuracy',keras.metrics.Precision()]) #,keras.metrics.Precision(),keras.metrics.Recall()
  return model

"""# **Train Model**"""

import time
class TimeHistory(keras.callbacks.Callback):
    def on_train_begin(self, logs={}):
        self.times = []

    def on_epoch_begin(self, batch, logs={}):
        self.epoch_time_start = time.time()

    def on_epoch_end(self, batch, logs={}):
        self.times.append(time.time() - self.epoch_time_start)

def train_model(x_train, y_train,x_test, y_test, hp, stage, epochs=50):
  
  # before = timeit.default_timer()
  if stage=='basic':
    model = build_model(hp[1],x_train.shape[1],y_train.shape[1])
    # print("Train model with lr: {}".format(hp[1]))
    global reduce_lr_epoch
    reduce_lr_epoch = hp[0]
    callbacks_lr = LearningRateScheduler(lr_scheduler_basic, verbose=1)
  if stage=='warm':
    model = build_model(hp[2],x_train.shape[1],y_train.shape[1])
    # print("Train model with lr: {}".format(hp[1]))
    global tmult
    tmult = hp[1]
    global ti
    ti = hp[0]
    callbacks_lr = LearningRateScheduler(lr_scheduler_warm, verbose=1)
    
  time_callback = TimeHistory()
  history = model.fit(x_train,y_train, validation_data=(x_test, y_test), batch_size=256, epochs=epochs, verbose=1,callbacks=[time_callback, callbacks_lr])

  history.history['train_time'] = time_callback.times
  return history

# basic_model = build_model_stage3()
# basic_model = build_model_stage1(hp)
# basic_model.fit(x_train,y_train, validation_data=(x_test, y_test), batch_size=32, epochs=50, verbose=1)
# train_history = train_model(x_train,y_train,x_test, y_test,hp_basic[0], stage='basic', epochs=15)
# print("times: {}", format(train_history.history['train_time']))

# train_history.history['val_accuracy'][-1]

def hp_search(x_train_inner, x_val_inner, y_train_inner, y_val_inner, hp, stage_name):
    # Return a tuple length len(hp) with each field contain the val accuracy of the corresponding hp
    val_acc = []
    counter = 0
    # for hyperparameters in hp:
    # print(hp[0:5])
    for hyperparameters in hp:
      print("hp_search counter:", counter)
      train_history = train_model(x_train_inner,y_train_inner,x_val_inner, y_val_inner, hyperparameters, stage=stage_name, epochs=50)
      # print("times: {}", format(train_history.history['train_time']))
      val_acc.append(train_history.history['val_accuracy'][-1])
      
      counter+=1
    return val_acc

def run(x,y):
  stage_name = ["basic","warm","improved"]
  cv_outer = KFold(n_splits=10, shuffle=True, random_state=1)
  # print(cv_outer)
  # X_train_kfold, x_val, y_train__kfold, y_val = train_test_split(x_train, y_train, test_size=0.2, random_state=42)
  i =0
  save_best_models=[]
  save_best_hp=[]
  for train_index, test_index in cv_outer.split(x,y):
    i += 1
    # print("train_{} [shape:{}]: {}".format(i,len(train_index),train_index))
    # print("test_{} [shape:{}]: {}".format(i,len(test_index),test_index))
    x_train, x_test = x[train_index], x[test_index]
    y_train, y_test = y[train_index], y[test_index]
    cv_inner = KFold(n_splits=3, shuffle=False, random_state=1)
    val_acc_inner = {}
    for j, (train_inner_idx, val_inner_idx) in enumerate(cv_inner.split(x_train, y_train)):
      print("10kfold: ",i , "3kfold: ",j)
      # print("train_inner_{} [shape:{}]: {}".format(j,len(train_inner_idx),train_inner_idx))
      # print("val_inner_{} [shape:{}]: {}".format(j,len(val_inner_idx),val_inner_idx))
      x_train_inner, x_val_inner = x[train_inner_idx], x[val_inner_idx]
      y_train_inner, y_val_inner = y[train_inner_idx], y[val_inner_idx]
      # rs = random_search(x_train_inner, y_train_inner,x_val_inner, y_val_inner)
      # print("hp[stage_name[0]]: ",hp[stage_name[0]])
      val_acc_inner[j] = (hp_search(x_train_inner, x_val_inner, y_train_inner, y_val_inner, hp[stage_name[1]], stage_name[1]))
    # print("val: ",val_acc_inner[0][0])
    # Do average between lists
    val_acc_inner_avg = []
    for k in range(0,len(val_acc_inner)):
      print("k: ", k)
      val_acc_inner_avg.append((val_acc_inner[0][k]+val_acc_inner[1][k]+val_acc_inner[2][k])/3)
    max_acc = max(val_acc_inner_avg)
    max_index_of_acc = val_acc_inner_avg.index(max_acc)
    best_hp = hp[stage_name[1]][max_index_of_acc]
    best_train_history = train_model(x_train,y_train,x_test, y_test, best_hp, stage='warm', epochs=50)
    save_best_models.append(best_train_history.history)
    save_best_hp.append(best_hp)
  return save_best_models,save_best_hp


  # Find max val acc and take the corresponding hp value from hp list
  # Train with the best hp value on al the training data: x_train, x_test y_train, y_test
  # val_acc_per_inner_split 
# Average 
  # val_split_acc